# -*- coding: utf-8 -*-
"""world_happiness_prediction_portuguese

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ql1BBYWHulHJcbApLcuPG_iQYN1ltlR
"""

# Importando as bibliotecas necessárias

import os
import zipfile
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Caminho para o arquivo ZIP
zip_file_path = "world-happiness-data-2024.zip"
# Descompactando o arquivo
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(".")

# Renomeando o arquivo descompactado para o nome correto.
os.rename("World Happiness Report 2024.csv", "world_happiness_2024.csv")

# Carregando e visualizando os dados

df = pd.read_csv("world_happiness_2024.csv")
df.head()

"""# Análise exploratória dos dados"""

df.info()

df.describe()

print("Colunas do Dataset:")
print(df.columns)

print("\nValores ausentes por coluna:")
print(df.isnull().sum())

print("Tipos de dados no dataset:")
print(df.dtypes)

#Verificar a distribuição do score de felicidade dos países (teste)
plt.figure(figsize=(8, 5))
sns.histplot(df['Life Ladder'], kde=True)
plt.title("Distribuição do Life Ladder (Happiness Score)")
plt.xlabel("Life Ladder")
plt.ylabel("Frequência")
plt.show()

"""# Tratamento de valores ausentes"""

# Selecionar apenas colunas numéricas
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Preencher valores ausentes nas colunas numéricas com a média
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

print("Valores ausentes após o tratamento:")
print(df.isnull().sum())

"""# Análise de Correlação"""

# Selecionar apenas colunas numéricas para calcular a correlação
numeric_columns = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numeric_columns.corr()

# Exibir a matriz de correlação
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matriz de Correlação")
plt.show()

"""A matriz de correlação mostra como cada variável numérica está relacionada às outras. Os valores variam entre:

+1: Correlação perfeita positiva (quando uma variável aumenta, a outra também aumenta).

0: Sem correlação (as variáveis não estão relacionadas).

-1: Correlação perfeita negativa (quando uma variável aumenta, a outra diminui).
"""

# Ordenar correlações com Life Ladder
correlation_with_life_ladder = correlation_matrix["Life Ladder"].sort_values(ascending=False)

print("Correlação com Life Ladder:")
print(correlation_with_life_ladder)

"""A análise das correlações com o Life Ladder (score de felicidade) revelou os seguintes insights:

Variáveis positivamente correlacionadas
- Log GDP per capita (0.77):
A maior correlação positiva, sugerindo que países com maior PIB per capita tendem a ter maiores níveis de felicidade.
- Social support (0.72):
O suporte social é outro fator crítico para a felicidade.
- Healthy life expectancy at birth (0.71):
A expectativa de vida saudável está fortemente relacionada à felicidade.
- Freedom to make life choices (0.53):
Liberdade para tomar decisões pessoais contribui significativamente para a felicidade.
- Positive affect (0.51):
Emoções positivas, como alegria, estão correlacionadas com níveis mais altos de felicidade.

Variáveis negativamente correlacionadas

- Perceptions of corruption (-0.42):
A percepção de corrupção tem uma correlação negativa significativa, indicando que quanto maior a corrupção percebida, menor é a felicidade.
- Negative affect (-0.35):
Emoções negativas, como tristeza ou raiva, também diminuem os níveis de felicidade.

# Modelagem preditiva
"""

# Selecionando as variáveis independentes (features) mais relevantes
features = ['Log GDP per capita', 'Social support', 'Healthy life expectancy at birth',
            'Freedom to make life choices', 'Positive affect', 'Perceptions of corruption']

# Variável dependente (target)
target = 'Life Ladder'

# Criando os DataFrames X (variáveis independentes) e y (variável dependente)
X = df[features]
y = df[target]

# Dividindo os dados em treino (80%) e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Verificando os tamanhos dos conjuntos
print(f"Tamanho do conjunto de treino: {X_train.shape}")
print(f"Tamanho do conjunto de teste: {X_test.shape}")

# Criando e ajustando o modelo
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Fazendo previsões no conjunto de teste
y_pred_lr = model_lr.predict(X_test)

# Avaliando o modelo com métricas de erro e precisão
from sklearn.metrics import mean_squared_error, r2_score

# Calculando as métricas
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

# Exibindo os resultados
print(f"MSE (Linear Regression): {mse_lr:.2f}")
print(f"R² Score (Linear Regression): {r2_lr:.2f}")

# Visualizando os valores reais vs. previsões
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_lr, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Linha de referência
plt.title("Valores Reais vs Previsões - Regressão Linear")
plt.xlabel("Valores Reais")
plt.ylabel("Previsões")
plt.show()

"""Interpretação do gráfico:

O gráfico mostra uma boa relação entre os valores reais (y_test) e as previsões (y_pred_lr), especialmente próximo à linha de referência (linha vermelha).

"""